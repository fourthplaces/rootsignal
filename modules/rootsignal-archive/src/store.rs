// Postgres persistence for archive content types. Internal to the archive crate.
// No platform knowledge — only universal content types.

use chrono::{DateTime, Utc};
use sqlx::PgPool;
use tracing::warn;
use uuid::Uuid;

use rootsignal_common::types::{
    ArchiveFile, ArchivedFeed, ArchivedPage, ArchivedSearchResults, FeedItem, LongVideo, Post,
    SearchResult, ShortVideo, Source, Story,
};

use crate::error::Result;

#[derive(Clone)]
pub(crate) struct Store {
    pool: PgPool,
}

// --- Insert types (no id/fetched_at — generated by DB) ---

pub(crate) struct InsertFile {
    pub url: String,
    pub content_hash: String,
    pub title: Option<String>,
    pub mime_type: String,
    pub duration: Option<f64>,
    pub page_count: Option<i32>,
    pub text: Option<String>,
    pub text_language: Option<String>,
}

pub(crate) struct InsertPost {
    pub source_id: Uuid,
    pub content_hash: String,
    pub text: Option<String>,
    pub author: Option<String>,
    pub location: Option<String>,
    pub engagement: Option<serde_json::Value>,
    pub published_at: Option<DateTime<Utc>>,
    pub permalink: Option<String>,
    pub mentions: Vec<String>,
    pub hashtags: Vec<String>,
    pub media_type: Option<String>,
    pub platform_id: Option<String>,
}

pub(crate) struct InsertStory {
    pub source_id: Uuid,
    pub content_hash: String,
    pub text: Option<String>,
    pub location: Option<String>,
    pub expires_at: Option<DateTime<Utc>>,
    pub permalink: Option<String>,
}

pub(crate) struct InsertShortVideo {
    pub source_id: Uuid,
    pub content_hash: String,
    pub text: Option<String>,
    pub location: Option<String>,
    pub engagement: Option<serde_json::Value>,
    pub published_at: Option<DateTime<Utc>>,
    pub permalink: Option<String>,
}

pub(crate) struct InsertLongVideo {
    pub source_id: Uuid,
    pub content_hash: String,
    pub text: Option<String>,
    pub engagement: Option<serde_json::Value>,
    pub published_at: Option<DateTime<Utc>>,
    pub permalink: Option<String>,
}

pub(crate) struct InsertPage {
    pub source_id: Uuid,
    pub content_hash: String,
    pub markdown: String,
    pub title: Option<String>,
    pub links: Vec<String>,
}

pub(crate) struct InsertFeed {
    pub source_id: Uuid,
    pub content_hash: String,
    pub items: serde_json::Value,
    pub title: Option<String>,
}

pub(crate) struct InsertSearchResults {
    pub source_id: Uuid,
    pub content_hash: String,
    pub query: String,
    pub results: serde_json::Value,
}

impl Store {
    pub(crate) fn new(pool: PgPool) -> Self {
        Self { pool }
    }

    // --- Sources ---

    /// Get or create a source by normalized URL.
    pub(crate) async fn upsert_source(&self, url: &str) -> Result<Source> {
        let row = sqlx::query_as::<_, (Uuid, String, DateTime<Utc>)>(
            r#"
            INSERT INTO sources (url) VALUES ($1)
            ON CONFLICT (url) DO UPDATE SET url = EXCLUDED.url
            RETURNING id, url, created_at
            "#,
        )
        .bind(url)
        .fetch_one(&self.pool)
        .await?;

        Ok(Source {
            id: row.0,
            url: row.1,
            created_at: row.2,
        })
    }

    // --- Source content types (freshness tracking) ---

    pub(crate) async fn increment_fetch_count(&self, source_id: Uuid) -> Result<()> {
        sqlx::query("UPDATE sources SET fetch_count = fetch_count + 1 WHERE id = $1")
            .bind(source_id)
            .execute(&self.pool)
            .await?;
        Ok(())
    }

    pub(crate) async fn update_last_scraped(
        &self,
        source_id: Uuid,
        content_type: &str,
    ) -> Result<()> {
        sqlx::query(
            r#"
            INSERT INTO source_content_types (source_id, content_type, last_scraped_at)
            VALUES ($1, $2, now())
            ON CONFLICT (source_id, content_type)
            DO UPDATE SET last_scraped_at = now()
            "#,
        )
        .bind(source_id)
        .bind(content_type)
        .execute(&self.pool)
        .await?;
        Ok(())
    }

    pub(crate) async fn get_last_scraped(
        &self,
        source_id: Uuid,
        content_type: &str,
    ) -> Result<Option<DateTime<Utc>>> {
        let row = sqlx::query_scalar::<_, DateTime<Utc>>(
            r#"
            SELECT last_scraped_at FROM source_content_types
            WHERE source_id = $1 AND content_type = $2
            "#,
        )
        .bind(source_id)
        .bind(content_type)
        .fetch_optional(&self.pool)
        .await?;
        Ok(row)
    }

    // --- Files ---

    /// Upsert a file by (url, content_hash). Returns existing row if already present.
    pub(crate) async fn upsert_file(&self, f: &InsertFile) -> Result<ArchiveFile> {
        let row = sqlx::query_as::<_, (Uuid, String, String, DateTime<Utc>, Option<String>, String, Option<f64>, Option<i32>, Option<String>, Option<String>)>(
            r#"
            INSERT INTO files (url, content_hash, title, mime_type, duration, page_count, text, text_language)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            ON CONFLICT (url, content_hash) DO UPDATE SET url = EXCLUDED.url
            RETURNING id, url, content_hash, fetched_at, title, mime_type, duration, page_count, text, text_language
            "#,
        )
        .bind(&f.url)
        .bind(&f.content_hash)
        .bind(&f.title)
        .bind(&f.mime_type)
        .bind(f.duration)
        .bind(f.page_count)
        .bind(&f.text)
        .bind(&f.text_language)
        .fetch_one(&self.pool)
        .await?;

        Ok(ArchiveFile {
            id: row.0,
            url: row.1,
            content_hash: row.2,
            fetched_at: row.3,
            title: row.4,
            mime_type: row.5,
            duration: row.6,
            page_count: row.7,
            text: row.8,
            text_language: row.9,
        })
    }

    /// Update the text/text_language on a file (after text analysis).
    pub(crate) async fn update_file_text(
        &self,
        file_id: Uuid,
        text: &str,
        language: Option<&str>,
    ) -> Result<()> {
        sqlx::query(
            r#"
            UPDATE files SET text = $1, text_language = $2 WHERE id = $3
            "#,
        )
        .bind(text)
        .bind(language)
        .bind(file_id)
        .execute(&self.pool)
        .await?;
        Ok(())
    }

    // --- Attachments ---

    /// Link files to a content record. `file_positions` is (file_id, position).
    pub(crate) async fn insert_attachments(
        &self,
        parent_type: &str,
        parent_id: Uuid,
        file_positions: &[(Uuid, i32)],
    ) -> Result<()> {
        for (file_id, position) in file_positions {
            sqlx::query(
                r#"
                INSERT INTO attachments (parent_type, parent_id, file_id, position)
                VALUES ($1, $2, $3, $4)
                "#,
            )
            .bind(parent_type)
            .bind(parent_id)
            .bind(file_id)
            .bind(position)
            .execute(&self.pool)
            .await?;
        }
        Ok(())
    }

    /// Get files attached to a content record.
    pub(crate) async fn get_attachments(
        &self,
        parent_type: &str,
        parent_id: Uuid,
    ) -> Result<Vec<ArchiveFile>> {
        let rows = sqlx::query_as::<_, (Uuid, String, String, DateTime<Utc>, Option<String>, String, Option<f64>, Option<i32>, Option<String>, Option<String>)>(
            r#"
            SELECT f.id, f.url, f.content_hash, f.fetched_at, f.title, f.mime_type,
                   f.duration, f.page_count, f.text, f.text_language
            FROM files f
            JOIN attachments a ON a.file_id = f.id
            WHERE a.parent_type = $1 AND a.parent_id = $2
            ORDER BY a.position ASC
            "#,
        )
        .bind(parent_type)
        .bind(parent_id)
        .fetch_all(&self.pool)
        .await?;

        Ok(rows
            .into_iter()
            .map(|r| ArchiveFile {
                id: r.0,
                url: r.1,
                content_hash: r.2,
                fetched_at: r.3,
                title: r.4,
                mime_type: r.5,
                duration: r.6,
                page_count: r.7,
                text: r.8,
                text_language: r.9,
            })
            .collect())
    }

    // --- Posts ---

    pub(crate) async fn insert_post(&self, p: &InsertPost) -> Result<Uuid> {
        let id = sqlx::query_scalar::<_, Uuid>(
            r#"
            INSERT INTO posts (source_id, content_hash, text, author, location, engagement, published_at, permalink, mentions, hashtags, media_type, platform_id)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
            RETURNING id
            "#,
        )
        .bind(p.source_id)
        .bind(&p.content_hash)
        .bind(&p.text)
        .bind(&p.author)
        .bind(&p.location)
        .bind(&p.engagement)
        .bind(p.published_at)
        .bind(&p.permalink)
        .bind(&p.mentions)
        .bind(&p.hashtags)
        .bind(&p.media_type)
        .bind(&p.platform_id)
        .fetch_one(&self.pool)
        .await?;
        Ok(id)
    }

    pub(crate) async fn get_posts(&self, source_id: Uuid, limit: u32) -> Result<Vec<Post>> {
        // 14 columns — large tuple, but avoids a custom FromRow derive.
        #[allow(clippy::type_complexity)]
        let rows = sqlx::query_as::<_, (Uuid, Uuid, DateTime<Utc>, String, Option<String>, Option<String>, Option<String>, Option<serde_json::Value>, Option<DateTime<Utc>>, Option<String>, Vec<String>, Vec<String>, Option<String>, Option<String>)>(
            r#"
            SELECT id, source_id, fetched_at, content_hash, text, author, location, engagement,
                   published_at, permalink, mentions, hashtags, media_type, platform_id
            FROM posts WHERE source_id = $1
            ORDER BY fetched_at DESC LIMIT $2
            "#,
        )
        .bind(source_id)
        .bind(limit as i64)
        .fetch_all(&self.pool)
        .await?;

        let mut posts = Vec::with_capacity(rows.len());
        for r in rows {
            let attachments = self.get_attachments("posts", r.0).await?;
            posts.push(Post {
                id: r.0,
                source_id: r.1,
                fetched_at: r.2,
                content_hash: r.3,
                text: r.4,
                author: r.5,
                location: r.6,
                engagement: r.7,
                published_at: r.8,
                permalink: r.9,
                mentions: r.10,
                hashtags: r.11,
                media_type: r.12,
                platform_id: r.13,
                attachments,
            });
        }
        Ok(posts)
    }

    // --- Stories ---

    pub(crate) async fn insert_story(&self, s: &InsertStory) -> Result<Uuid> {
        let id = sqlx::query_scalar::<_, Uuid>(
            r#"
            INSERT INTO stories (source_id, content_hash, text, location, expires_at, permalink)
            VALUES ($1, $2, $3, $4, $5, $6)
            RETURNING id
            "#,
        )
        .bind(s.source_id)
        .bind(&s.content_hash)
        .bind(&s.text)
        .bind(&s.location)
        .bind(s.expires_at)
        .bind(&s.permalink)
        .fetch_one(&self.pool)
        .await?;
        Ok(id)
    }

    pub(crate) async fn get_stories(&self, source_id: Uuid) -> Result<Vec<Story>> {
        let rows = sqlx::query_as::<_, (Uuid, Uuid, DateTime<Utc>, String, Option<String>, Option<String>, Option<DateTime<Utc>>, Option<String>)>(
            r#"
            SELECT id, source_id, fetched_at, content_hash, text, location, expires_at, permalink
            FROM stories WHERE source_id = $1
            ORDER BY fetched_at DESC
            "#,
        )
        .bind(source_id)
        .fetch_all(&self.pool)
        .await?;

        let mut stories = Vec::with_capacity(rows.len());
        for r in rows {
            let attachments = self.get_attachments("stories", r.0).await?;
            stories.push(Story {
                id: r.0,
                source_id: r.1,
                fetched_at: r.2,
                content_hash: r.3,
                text: r.4,
                location: r.5,
                expires_at: r.6,
                permalink: r.7,
                attachments,
            });
        }
        Ok(stories)
    }

    // --- Short Videos ---

    pub(crate) async fn insert_short_video(&self, v: &InsertShortVideo) -> Result<Uuid> {
        let id = sqlx::query_scalar::<_, Uuid>(
            r#"
            INSERT INTO short_videos (source_id, content_hash, text, location, engagement, published_at, permalink)
            VALUES ($1, $2, $3, $4, $5, $6, $7)
            RETURNING id
            "#,
        )
        .bind(v.source_id)
        .bind(&v.content_hash)
        .bind(&v.text)
        .bind(&v.location)
        .bind(&v.engagement)
        .bind(v.published_at)
        .bind(&v.permalink)
        .fetch_one(&self.pool)
        .await?;
        Ok(id)
    }

    pub(crate) async fn get_short_videos(
        &self,
        source_id: Uuid,
        limit: u32,
    ) -> Result<Vec<ShortVideo>> {
        let rows = sqlx::query_as::<_, (Uuid, Uuid, DateTime<Utc>, String, Option<String>, Option<String>, Option<serde_json::Value>, Option<DateTime<Utc>>, Option<String>)>(
            r#"
            SELECT id, source_id, fetched_at, content_hash, text, location, engagement, published_at, permalink
            FROM short_videos WHERE source_id = $1
            ORDER BY fetched_at DESC LIMIT $2
            "#,
        )
        .bind(source_id)
        .bind(limit as i64)
        .fetch_all(&self.pool)
        .await?;

        let mut videos = Vec::with_capacity(rows.len());
        for r in rows {
            let attachments = self.get_attachments("short_videos", r.0).await?;
            videos.push(ShortVideo {
                id: r.0,
                source_id: r.1,
                fetched_at: r.2,
                content_hash: r.3,
                text: r.4,
                location: r.5,
                engagement: r.6,
                published_at: r.7,
                permalink: r.8,
                attachments,
            });
        }
        Ok(videos)
    }

    // --- Long Videos ---

    pub(crate) async fn insert_long_video(&self, v: &InsertLongVideo) -> Result<Uuid> {
        let id = sqlx::query_scalar::<_, Uuid>(
            r#"
            INSERT INTO long_videos (source_id, content_hash, text, engagement, published_at, permalink)
            VALUES ($1, $2, $3, $4, $5, $6)
            RETURNING id
            "#,
        )
        .bind(v.source_id)
        .bind(&v.content_hash)
        .bind(&v.text)
        .bind(&v.engagement)
        .bind(v.published_at)
        .bind(&v.permalink)
        .fetch_one(&self.pool)
        .await?;
        Ok(id)
    }

    pub(crate) async fn get_long_videos(
        &self,
        source_id: Uuid,
        limit: u32,
    ) -> Result<Vec<LongVideo>> {
        let rows = sqlx::query_as::<_, (Uuid, Uuid, DateTime<Utc>, String, Option<String>, Option<serde_json::Value>, Option<DateTime<Utc>>, Option<String>)>(
            r#"
            SELECT id, source_id, fetched_at, content_hash, text, engagement, published_at, permalink
            FROM long_videos WHERE source_id = $1
            ORDER BY fetched_at DESC LIMIT $2
            "#,
        )
        .bind(source_id)
        .bind(limit as i64)
        .fetch_all(&self.pool)
        .await?;

        let mut videos = Vec::with_capacity(rows.len());
        for r in rows {
            let attachments = self.get_attachments("long_videos", r.0).await?;
            videos.push(LongVideo {
                id: r.0,
                source_id: r.1,
                fetched_at: r.2,
                content_hash: r.3,
                text: r.4,
                engagement: r.5,
                published_at: r.6,
                permalink: r.7,
                attachments,
            });
        }
        Ok(videos)
    }

    // --- Pages ---

    pub(crate) async fn insert_page(&self, p: &InsertPage) -> Result<Uuid> {
        let id = sqlx::query_scalar::<_, Uuid>(
            r#"
            INSERT INTO pages (source_id, content_hash, markdown, title, links)
            VALUES ($1, $2, $3, $4, $5)
            RETURNING id
            "#,
        )
        .bind(p.source_id)
        .bind(&p.content_hash)
        .bind(&p.markdown)
        .bind(&p.title)
        .bind(&p.links)
        .fetch_one(&self.pool)
        .await?;
        Ok(id)
    }

    pub(crate) async fn get_page(&self, source_id: Uuid) -> Result<Option<ArchivedPage>> {
        let row = sqlx::query_as::<_, (Uuid, Uuid, DateTime<Utc>, String, String, String, Option<String>, Vec<String>)>(
            r#"
            SELECT id, source_id, fetched_at, content_hash, raw_html, markdown, title, links
            FROM pages WHERE source_id = $1
            ORDER BY fetched_at DESC LIMIT 1
            "#,
        )
        .bind(source_id)
        .fetch_optional(&self.pool)
        .await?;

        Ok(row.map(|r| ArchivedPage {
            id: r.0,
            source_id: r.1,
            fetched_at: r.2,
            content_hash: r.3,
            raw_html: r.4,
            markdown: r.5,
            title: r.6,
            links: r.7,
            published_at: None, // Computed at scrape time from raw_html, not stored in DB
        }))
    }

    // --- Feeds ---

    pub(crate) async fn insert_feed(&self, f: &InsertFeed) -> Result<Uuid> {
        let id = sqlx::query_scalar::<_, Uuid>(
            r#"
            INSERT INTO feeds (source_id, content_hash, items, title)
            VALUES ($1, $2, $3, $4)
            RETURNING id
            "#,
        )
        .bind(f.source_id)
        .bind(&f.content_hash)
        .bind(&f.items)
        .bind(&f.title)
        .fetch_one(&self.pool)
        .await?;
        Ok(id)
    }

    pub(crate) async fn get_feed(&self, source_id: Uuid) -> Result<Option<ArchivedFeed>> {
        let row = sqlx::query_as::<_, (Uuid, Uuid, DateTime<Utc>, String, serde_json::Value, Option<String>)>(
            r#"
            SELECT id, source_id, fetched_at, content_hash, items, title
            FROM feeds WHERE source_id = $1
            ORDER BY fetched_at DESC LIMIT 1
            "#,
        )
        .bind(source_id)
        .fetch_optional(&self.pool)
        .await?;

        Ok(row.map(|r| {
            let items: Vec<FeedItem> = serde_json::from_value(r.4).unwrap_or_default();
            ArchivedFeed {
                id: r.0,
                source_id: r.1,
                fetched_at: r.2,
                content_hash: r.3,
                items,
                title: r.5,
            }
        }))
    }

    // --- Search Results ---

    pub(crate) async fn insert_search_results(&self, s: &InsertSearchResults) -> Result<Uuid> {
        let id = sqlx::query_scalar::<_, Uuid>(
            r#"
            INSERT INTO search_results (source_id, content_hash, query, results)
            VALUES ($1, $2, $3, $4)
            RETURNING id
            "#,
        )
        .bind(s.source_id)
        .bind(&s.content_hash)
        .bind(&s.query)
        .bind(&s.results)
        .fetch_one(&self.pool)
        .await?;
        Ok(id)
    }

    pub(crate) async fn get_search_results(
        &self,
        source_id: Uuid,
    ) -> Result<Option<ArchivedSearchResults>> {
        let row = sqlx::query_as::<_, (Uuid, Uuid, DateTime<Utc>, String, String, serde_json::Value)>(
            r#"
            SELECT id, source_id, fetched_at, content_hash, query, results
            FROM search_results WHERE source_id = $1
            ORDER BY fetched_at DESC LIMIT 1
            "#,
        )
        .bind(source_id)
        .fetch_optional(&self.pool)
        .await?;

        Ok(row.map(|r| {
            let results: Vec<SearchResult> = serde_json::from_value(r.5).unwrap_or_default();
            ArchivedSearchResults {
                id: r.0,
                source_id: r.1,
                fetched_at: r.2,
                content_hash: r.3,
                query: r.4,
                results,
            }
        }))
    }
}
